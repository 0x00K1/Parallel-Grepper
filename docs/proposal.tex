\documentclass[12pt,a4paper]{article}

% ============================================================
% PACKAGE IMPORTS
% ============================================================
\usepackage{fvextra}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}  % For checkmark symbol
\usepackage{caption}
\usepackage{setspace}  % For line spacing control
\usepackage{tocloft}   % For table of contents formatting

% ============================================================
% COLOR DEFINITIONS
% ============================================================
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{iaublue}{rgb}{0.0,0.3,0.6}

% ============================================================
% CODE LISTING STYLE
% ============================================================
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    framesep=3pt,
    xleftmargin=10pt,
    xrightmargin=5pt
}
\lstset{style=mystyle}

% ============================================================
% HEADER AND FOOTER CONFIGURATION
% ============================================================
\setlength{\headheight}{14pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small ARTI503 - Parallel Computer Architecture}
\fancyhead[R]{\small Group 3}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% ============================================================
% HYPERLINK CONFIGURATION
% ============================================================
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Parallel Text Processing Project Proposal},
    pdfauthor={Group 3 - IAU},
    pdfsubject={Parallel Computer Architecture},
    pdfkeywords={Parallel Computing, OpenMP, Text Processing, Word Frequency}
}

% ============================================================
% SECTION TITLE FORMATTING
% ============================================================
\titleformat{\section}
    {\normalfont\Large\bfseries\color{iaublue}}
    {\thesection}{1em}{}
    [\vspace{0.5ex}\titlerule]

\titleformat{\subsection}
    {\normalfont\large\bfseries}
    {\thesubsection}{1em}{}

\titleformat{\subsubsection}
    {\normalfont\normalsize\bfseries}
    {\thesubsubsection}{1em}{}

% Spacing adjustments
\titlespacing*{\section}{0pt}{12pt plus 4pt minus 2pt}{6pt plus 2pt minus 2pt}
\titlespacing*{\subsection}{0pt}{10pt plus 4pt minus 2pt}{4pt plus 2pt minus 2pt}
\titlespacing*{\subsubsection}{0pt}{8pt plus 4pt minus 2pt}{4pt plus 2pt minus 2pt}

% Set line spacing
\setstretch{1.15}

% ============================================================
% TABLE OF CONTENTS FORMATTING
% ============================================================
% Reduce spacing between TOC entries
\setlength{\cftbeforesecskip}{2pt}
\setlength{\cftbeforesubsecskip}{1pt}
\setlength{\cftbeforesubsubsecskip}{0pt}

% ============================================================
% DOCUMENT BEGIN
% ============================================================
\begin{document}

% ============================================================
% TITLE PAGE
% ============================================================
\begin{titlepage}
    \centering
    \vspace*{1.5cm}
    
    {\huge\bfseries Group3@IAU-ARTI503:\\[0.5cm]
    Parallel Text Processing and\\[0.3cm]
    Word Frequency Counter\\[3cm]}
    
    {\large
    Abdullah Albattat\textsuperscript{1}, 
    Hussain Alghubari\textsuperscript{1}, 
    Muhannad Almahmoud\textsuperscript{1},\\[0.3cm]
    Khalid Alghamdi\textsuperscript{1}, and 
    Abdullah Aladwani\textsuperscript{1}\\[1.5cm]
    
    \textsuperscript{1}Imam Abdulrahman Bin Faisal University\\
    College of Computer Science and Information Technology\\
    Dammam, Saudi Arabia\\[2.5cm]
    
    ARTI503 - Parallel Computer Architecture\\[0.3cm]
    December 10, 2025
    }
    
    \vfill
\end{titlepage}

% ============================================================
% TABLE OF CONTENTS
% ============================================================
\footnotesize  % Smaller font size for table of contents
\setlength{\parskip}{0pt}  % Remove extra spacing between entries
\begin{spacing}{0.9}  % Reduce line spacing for TOC
\tableofcontents
\end{spacing}
\normalsize  % Restore normal font size
\setstretch{1.15}  % Restore normal line spacing
\newpage

% ============================================================
\section{Introduction}
% ============================================================

\subsection{Objective and Purpose}

\subsubsection{Problem Statement}

In the era of big data and digitalization, text processing has become a fundamental computational task across many domains like:

\begin{itemize}[leftmargin=*, itemsep=4pt]
    \item \textbf{Search Engines:} Indexing billions of web pages
    
    \item \textbf{Social Media Analytics:} Processing real-time streams of user-generated content
    
    \item \textbf{Natural Language Processing:} Processing large corpora for machine learning models
    
    \item \textbf{Document Management Systems:} Managing and analyzing enterprise documents
    
    \item \textbf{Log Analysis:} Processing server and application logs for monitoring
\end{itemize}

\vspace{0.5cm}

Conventional sequential text-processing algorithms are severely limited for processing large-scale data. As file sizes increase from megabytes to gigabytes, single-threaded methods turn into major bottlenecks, leading to:

\begin{itemize}[leftmargin=*, itemsep=4pt]
    \item Unacceptable processing latency
    
    \item Underutilization of modern multi-core processors
    
    \item Poor scalability for growing data volumes
    
    \item Inability to meet real-time processing requirements
\end{itemize}

\newpage
\subsubsection{Project Purpose}

The primary purpose of this project is to \textbf{design, implement, and evaluate} a parallel word frequency counter that leverages multi-core processors to dramatically improve the performance of text processing.

\vspace{0.3cm}
\noindent\textbf{Specific Objectives:}

\begin{enumerate}[leftmargin=*, itemsep=6pt]
    \item \textbf{Implement Sequential Baseline:} Create a sequential word counter to establish performance baselines
    
    \item \textbf{Develop Parallel Solution:} Develop an OpenMP-based parallel solution through shared-memory programming
    
    \item \textbf{Performance Analysis:} Measure and compare execution time, speedup, and efficiency across different configurations
    
    \item \textbf{Scalability Study:} Evaluate how performance scales with:
        \begin{itemize}[itemsep=2pt]
            \item Dataset size (10 MB to 100+ MB)
            \item Thread count (2, 4, 8, 16 threads)
        \end{itemize}
    
    \item \textbf{Optimization:} Identify and minimize synchronization overhead and load imbalance
    
    \item \textbf{Real-World Application:} Demonstrate practical benefits of parallelization in text analytics
\end{enumerate}

\vspace{0.5cm}

\subsubsection{Why This Problem Matters for Parallel Computing}

Word frequency counting is an ideal candidate for parallel computing education and research because:

\begin{itemize}[leftmargin=*, itemsep=4pt]
    \item \textbf{Computational Intensity:} Processing large text files involves millions of operations
    
    \item \textbf{Data Independence:} Text chunks can be processed independently
    
    \item \textbf{Clear Metrics:} Speedup and efficiency are easily measurable
    
    \item \textbf{Real-World Relevance:} Directly applicable to industry problems
    
    \item \textbf{Educational Value:} Demonstrates key parallel computing concepts (synchronization, load balancing, Amdahl's Law)
\end{itemize}

\newpage

\subsection{Code Selection and Justification}

\subsubsection{Why We Selected This Problem}

We selected the word frequency counter problem for parallelization because it exhibits \textbf{ideal characteristics} for demonstrating parallel computing principles:

\noindent\textbf{1. Computational Intensity:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Processing large text files (50-100 MB) involves \textbf{tens of millions of operations}
    \item Each word requires: reading, tokenization, normalization, and hash map insertion
    \item Sequential processing can take several seconds to minutes for large datasets
\end{itemize}

\vspace{0.4cm}
\noindent\textbf{2. Existence of Parallelizable Loops:}

The core algorithm contains \textbf{highly parallelizable loops}:

\begin{lstlisting}[language=C++, caption={Sequential Word Counting Loop - Main Bottleneck}]
// Main processing loop - SEQUENTIAL BOTTLENECK
while (file >> word) {
    // Step 1: Normalize word (lowercase, remove punctuation)
    normalized_word = normalize(word);  // CPU-intensive
    
    // Step 2: Update frequency map
    wordFreq[normalized_word]++;        // Hash map operation
    
    totalWords++;
}
\end{lstlisting}

This loop is executed \textbf{millions of times} and dominates execution time, making it the prime target for parallelization.

\vspace{0.4cm}
\noindent\textbf{3. Data Independence:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Text can be divided into independent chunks
    \item Each chunk can be processed in parallel without dependencies
    \item Only the final merge step requires synchronization
\end{itemize}

\newpage
\vspace{0.4cm}
\noindent\textbf{4. Multiple Parallelization Opportunities:}

\begin{table}[h]
\centering
\caption{Parallelization Opportunities in Word Frequency Counter}
\small
\begin{tabular}{|p{2.8cm}|p{4.8cm}|p{4.8cm}|}
\hline
\textbf{Component} & \textbf{Sequential Operation} & \textbf{Parallel Strategy} \\
\hline
File Reading & Read entire file sequentially & Divide into chunks, parallel read \\
\hline
Tokenization & Process words one-by-one & Each thread processes its chunk \\
\hline
Word Normalization & Sequential character processing & Independent per word \\
\hline
Frequency Counting & Single global hash map & Thread-local maps + merge \\
\hline
Result Sorting & Sequential sort & Parallel sort (if needed) \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{5. Presence of Different Data Types:}

The code involves multiple data types suitable for parallel processing:

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \texttt{std::string}: Text data requiring character-level operations
    \item \texttt{std::unordered\_map<string, unsigned long long>}: Hash table for frequency storage
    \item \texttt{unsigned long long}: Large integer counters
    \item \texttt{std::vector}: Dynamic arrays for storing results
\end{itemize}

\vspace{0.4cm}
\noindent\textbf{6. Conditional Operations:}

The algorithm includes if-conditions that benefit from parallel evaluation:
\begin{lstlisting}[language=C++, caption={Conditional Logic in Word Processing}]
// Multiple conditional checks per word
if (isalpha(c)) {  // Character validation
    normalized += tolower(c);
}

if (!normalized.empty()) {  // Empty word filtering
    wordFreq[normalized]++;
}

// Frequency threshold filtering
if (frequency > MIN_THRESHOLD) {
    results.push_back({word, frequency});
}
\end{lstlisting}

\newpage
\subsubsection{Suitability for OpenMP Parallelization}

OpenMP is \textbf{ideally suited} for this problem because:

\vspace{0.3cm}
\noindent\textbf{1. Shared-Memory Model:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Text data can be shared across threads
    \item Reduces memory overhead compared to distributed systems
    \item Efficient for single-machine processing
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{2. Parallel Directives:}

OpenMP provides directives perfect for our use case:

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \texttt{\#pragma omp parallel for}: Parallelize word processing loops
    \item \texttt{\#pragma omp critical}: Protect hash map updates
    \item \texttt{\#pragma omp reduction}: Combine thread-local results
    \item \texttt{\#pragma omp sections}: Parallel task distribution
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{3. Scheduling Options:}

Test different scheduling strategies:

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \texttt{static}: Equal chunk distribution
    \item \texttt{dynamic}: Load balancing for uneven workloads
    \item \texttt{guided}: Adaptive chunk sizing
\end{itemize}

\subsubsection{Computational Complexity Analysis}

\noindent\textbf{Time Complexity:}

\begin{itemize}[leftmargin=*, itemsep=4pt]
    \item \textbf{Sequential:} $O(n)$ where $n$ is the number of tokens (words).
    
    \item \textbf{Parallel (with $p$ threads):} $O\!\left(\frac{n}{p}\right) + O\!\left(U \log p\right)$
        \begin{itemize}[itemsep=2pt]
            \item $\frac{n}{p}$: Per-thread parsing, tokenization, normalization, and local counting.
            \item $U \log p$: Tree-style merge of $p$ thread-local frequency maps, where $U$ is the total number of distinct words.
        \end{itemize}
\end{itemize}

\vspace{0.3cm}

\noindent\textit{Note:} Using a single global hash map with locks can introduce heavy contention and degrade performance; the thread-local + merge strategy above minimizes this. If sorted output is required, add an extra $O(U \log U)$ post-processing term.

\newpage
\vspace{0.4cm}
\noindent\textbf{Expected Speedup:}

According to \textbf{Amdahl's Law}:

\begin{equation}
    S(p) = \frac{1}{(1-P) + \frac{P}{p}}
\end{equation}

\noindent where $P$ is the parallelizable fraction of the workload and $p$ is the number of processors/threads.

\subsubsection{Code Origin}

\noindent\textbf{Original Implementation:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item The sequential code is \textbf{originally developed} by our team for this project
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{References and Inspiration:}

\begin{enumerate}[leftmargin=*, itemsep=3pt]
    \item OpenMP official documentation and tutorials: \url{https://www.openmp.org/}
    \item Chapman, Barbara, et al. \textit{Using OpenMP: Portable Shared Memory Parallel Programming}. MIT Press, 2007.
    \item Course materials from ARTI503 - Parallel Computer Architecture
\end{enumerate}

\vspace{0.3cm}
\noindent\textbf{GitHub Repository:}

All code will be published at: \url{https://github.com/0x00K1/Parallel-Grepper}

\newpage
% ============================================================
\section{Sequential Benchmarking}
% ============================================================

\subsection{Benchmarking Methodology}

\subsubsection{Measurement Tools and Techniques}

To establish a reliable baseline for performance comparison, we implemented a comprehensive benchmarking framework using the following tools and methodologies:

\vspace{0.3cm}
\noindent\textbf{1. Time Measurement in C++ (High-Precision Timing):}

\begin{lstlisting}[language=C++, caption={High-Resolution Timer Implementation}]
#include <chrono>

// Start timing
auto startTime = std::chrono::high_resolution_clock::now();

// Execute word counting algorithm
WordMap wordFreq = countWordsFromFile(filename);

// End timing
auto endTime = std::chrono::high_resolution_clock::now();

// Calculate execution time in milliseconds
executionTime = std::chrono::duration<double, std::milli>(
    endTime - startTime
).count();
\end{lstlisting}

\noindent\textbf{Key Features:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \texttt{std::chrono::high\_resolution\_clock}: Provides microsecond-level precision
    \item Measures wall-clock time including I/O operations
    \item Minimal overhead from timing instrumentation
\end{itemize}

\vspace{0.4cm}
\noindent\textbf{2. Python Benchmarking Suite:}

We developed an automated benchmarking script (\texttt{run\_benchmarks.py}) that:

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Executes multiple runs (5 iterations per dataset) for statistical reliability
    \item Collects execution times through Python's \texttt{time.perf\_counter()}
    \item Calculates mean, median, standard deviation, min, and max times
    \item Outputs results in JSON, CSV, and formatted text formats
    \item Ensures consistent system state between runs
\end{itemize}

\newpage
\subsubsection{Test Environment Specifications}

\begin{table}[h]
\centering
\caption{Hardware and Software Configuration}
\begin{tabular}{|l|l|}
\hline
\textbf{Component} & \textbf{Specification} \\
\hline
Operating System & Windows 11 (64-bit) \\
\hline
Compiler & GCC 15.2.0 (MinGW-w64 POSIX UCRT) \\
\hline
Optimization Level & -O3 (Maximum optimization) \\
\hline
C++ Standard & C++17 \\
\hline
Python Version & Python 3.x \\
\hline
Benchmark Runs & 5 iterations per dataset \\
\hline
\end{tabular}
\end{table}

\subsubsection{Dataset Specifications}

We generated synthetic datasets with controlled characteristics to test scalability:

\begin{table}[h]
\centering
\caption{Test Dataset Specifications}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Dataset} & \textbf{File Size} & \textbf{Total Words} & \textbf{Unique Words} \\
\hline
test\_10mb.txt & 10.06 MB & 1,854,066 & 140 \\
\hline
test\_25mb.txt & 25.15 MB & 4,635,262 & 140 \\
\hline
test\_50mb.txt & 50.29 MB & 9,270,623 & 140 \\
\hline
test\_100mb.txt & 100.59 MB & 18,538,135 & 140 \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Dataset Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Vocabulary size: Fixed at 140 unique words (limitation: results may differ for real-world corpora with much larger vocabularies and different hash-map collision patterns)
    \item Word distribution: Zipf's law distribution (realistic text patterns)
    \item File sizes: 10×, 2.5×, 2×, and 2× scaling factors
    \item Purpose: Test linear scalability and identify bottlenecks
\end{itemize}

\newpage
\subsection{Performance Results}

\subsubsection{Execution Time Analysis}

The sequential word counter was benchmarked across all test datasets with 5 runs per dataset to ensure statistical reliability.

\begin{table}[h]
\centering
\caption{Sequential Performance Metrics}
\small
\begin{tabular}{|l|r|r|r|r|}
\hline
\textbf{Dataset} & \textbf{Mean Time (ms)} & \textbf{Std Dev (ms)} & \textbf{Min (ms)} & \textbf{Max (ms)} \\
\hline
test\_10mb.txt & 275.60 & 11.07 & 260.96 & 286.84 \\
\hline
test\_25mb.txt & 641.84 & 6.00 & 634.76 & 650.77 \\
\hline
test\_50mb.txt & 1,271.00 & 16.86 & 1,254.13 & 1,293.80 \\
\hline
test\_100mb.txt & 2,607.80 & 87.32 & 2,510.31 & 2,738.18 \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Key Observations:}

\begin{enumerate}[leftmargin=*, itemsep=3pt]
    \item \textbf{Linear Scaling:} Execution time scales linearly with file size
        \begin{itemize}[itemsep=2pt]
            \item 10 MB → 25 MB (2.5× size): 2.33× time increase
            \item 25 MB → 50 MB (2.0× size): 1.98× time increase
            \item 50 MB → 100 MB (2.0× size): 2.05× time increase
        \end{itemize}
    
    \item \textbf{Low Variance:} Standard deviation is consistently low (2.3\%-4.0\% of mean)
        \begin{itemize}[itemsep=2pt]
            \item Indicates stable, predictable performance
            \item Minimal impact from system background processes
        \end{itemize}
    
    \item \textbf{Consistent Performance:} The algorithm exhibits $O(n)$ complexity as expected
\end{enumerate}

\subsubsection{Throughput Analysis}

\begin{table}[h]
\centering
\caption{Processing Throughput Metrics}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Dataset} & \textbf{Throughput (MB/s)} & \textbf{Words/Second} & \textbf{Efficiency} \\
\hline
test\_10mb.txt & 36.50 & 6,727,403 & Baseline \\
\hline
test\_25mb.txt & 39.18 & 7,221,788 & +7.3\% \\
\hline
test\_50mb.txt & 39.57 & 7,293,975 & +8.4\% \\
\hline
test\_100mb.txt & 38.57 & 7,108,728 & +5.7\% \\
\hline
\textbf{Average} & \textbf{38.46} & \textbf{7,087,974} & --- \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Analysis:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Stable Throughput:} Consistent \textasciitilde38 MB/s across all dataset sizes
    \item \textbf{Slight Performance Gain:} Larger files show marginally better throughput
        \begin{itemize}[itemsep=2pt]
            \item Reason: Better cache utilization and amortized I/O overhead
            \item Effect: 5-8\% improvement from 10 MB to 50-100 MB files
        \end{itemize}
    \item \textbf{Word Processing Rate:} Consistently processes \textasciitilde7 million words per second
\end{itemize}

\subsubsection{Performance Visualization}

Figure~\ref{fig:performance_analysis} presents a comprehensive visualization of the sequential word counter's performance characteristics across four key dimensions:

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{performance_analysis.png}
\caption{Sequential Word Counter Performance Analysis: (Top-Left) Execution time vs file size showing linear scaling with standard deviation bands; (Top-Right) Processing throughput in MB/s demonstrating consistent performance; (Bottom-Left) Word processing rate showing \textasciitilde7 million words/second capability; (Bottom-Right) Execution time with error bars indicating measurement reliability.}
\label{fig:performance_analysis}
\end{figure}

\newpage
\noindent\textbf{Figure Interpretation:}

\begin{enumerate}[leftmargin=*, itemsep=3pt]
    \item \textbf{Top-Left (Execution Time vs File Size):}
        \begin{itemize}[itemsep=2pt]
            \item Shows clear linear relationship between file size and execution time
            \item Shaded region represents ±1 standard deviation
            \item Narrow bands indicate high measurement consistency
            \item Confirms $O(n)$ algorithmic complexity
        \end{itemize}
    
    \item \textbf{Top-Right (Processing Throughput):}
        \begin{itemize}[itemsep=2pt]
            \item Demonstrates stable throughput across all dataset sizes
            \item Average throughput: 38.46 MB/s
            \item Slight improvement with larger files due to cache effects
            \item No performance degradation with increasing file size
        \end{itemize}
    
    \item \textbf{Bottom-Left (Word Processing Rate):}
        \begin{itemize}[itemsep=2pt]
            \item Consistent processing rate of \textasciitilde7 million words/second
            \item Validates throughput measurements
            \item Shows excellent performance consistency
            \item Indicates efficient word tokenization and counting
        \end{itemize}
    
    \item \textbf{Bottom-Right (Execution Time with Error Bars):}
        \begin{itemize}[itemsep=2pt]
            \item Error bars represent standard deviation (5 runs per dataset)
            \item Small error bars indicate reliable measurements
            \item Minimal variance across all dataset sizes
            \item Confirms repeatability and stability of benchmarks
        \end{itemize}
\end{enumerate}

\vspace{0.3cm}
\noindent\textbf{Key Insights from Visualization:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Predictable Performance:} Linear scaling enables accurate time estimation for larger datasets
    \item \textbf{No Bottleneck Saturation:} Throughput remains stable, indicating no system-level bottlenecks
    \item \textbf{Measurement Reliability:} Low variance confirms benchmarking methodology validity
    \item \textbf{Optimization Potential:} Consistent behavior across scales suggests parallelization will be effective
\end{itemize}

\newpage
\subsection{Bottleneck Identification}

\subsubsection{Profiling Analysis}

Through detailed code analysis and execution time breakdown, we identified the following performance bottlenecks:

\vspace{0.3cm}
\noindent\textbf{1. File I/O Operations (Estimated: 25-30\% of execution time)}

\begin{lstlisting}[language=C++, caption={Sequential File Reading - First Major Bottleneck}]
std::ifstream file(filename);

// Sequential read - blocks until data available
while (file >> word) {  // I/O BOTTLENECK
    // Processing happens here
}
\end{lstlisting}

\noindent\textbf{Bottleneck Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item Single-threaded disk I/O
    \item Stream extraction operator (\texttt{>>}) performs buffered reads
    \item Limited by disk read speed and buffer size
    \item Cannot proceed to next word until current read completes
\end{itemize}

\newpage
\vspace{0.4cm}
\noindent\textbf{2. String Processing and Normalization (Estimated: 40-45\% of execution time)}

\begin{lstlisting}[language=C++, caption={Word Normalization - Primary Computational Bottleneck}]
std::string WordCounterSequential::normalizeWord(const std::string& word) {
    std::string normalized;
    normalized.reserve(word.length());
    
    // CHARACTER-BY-CHARACTER PROCESSING - MAIN BOTTLENECK
    for (char c : word) {
        if (std::isalpha(static_cast<unsigned char>(c))) {
            normalized += std::tolower(static_cast<unsigned char>(c));
        }
    }
    
    return normalized;
}
\end{lstlisting}

\noindent\textbf{Bottleneck Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Called once per word (18.5 million times for 100 MB file)
    \item Character-by-character validation and transformation
    \item Multiple function calls per character (\texttt{isalpha}, \texttt{tolower})
    \item String concatenation overhead
    \item \textbf{This is the most computationally intensive section}
\end{itemize}

\newpage
\noindent\textbf{3. Hash Map Operations (Estimated: 20-25\% of execution time)}

\begin{lstlisting}[language=C++, caption={Frequency Map Updates - Synchronization Bottleneck}]
WordMap wordFreq;  // std::unordered_map<std::string, unsigned long long>

while (file >> word) {
    std::string normalized = normalizeWord(word);
    
    if (!normalized.empty()) {
        wordFreq[normalized]++;  // HASH MAP BOTTLENECK
        totalWords++;
    }
}
\end{lstlisting}

\noindent\textbf{Bottleneck Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=0pt]
    \item Hash computation for each word lookup
    \item Potential hash collisions (though minimal with 140 unique words)
    \item Memory allocation for new entries
    \item Cache misses due to random memory access patterns
    \item \textbf{Critical section in parallel version - requires synchronization}
\end{itemize}

\vspace{0.4cm}
\noindent\textbf{4. Result Sorting (Estimated: 5-10\% of execution time)}

\begin{lstlisting}[language=C++, caption={Top Words Sorting - Post-Processing Overhead}]
std::vector<std::pair<std::string, unsigned long long>> 
WordCounterSequential::getTopWords(const WordMap& wordMap, int n) {
    // Convert map to vector
    std::vector<std::pair<std::string, unsigned long long>> wordVec(
        wordMap.begin(), wordMap.end()
    );
    
    // Sort by frequency (descending) - O(U log U)
    std::sort(wordVec.begin(), wordVec.end(),
        [](const auto& a, const auto& b) {
            return a.second > b.second;  // SORTING OVERHEAD
        }
    );
    
    return wordVec;
}
\end{lstlisting}

\noindent\textbf{Bottleneck Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item $O(U \log U)$ complexity where $U$ = unique words (140)
    \item Minimal impact due to small vocabulary size
    \item Would become significant with larger vocabularies (10K+ unique words)
\end{itemize}

\newpage
\subsubsection{Execution Time Breakdown}

Based on profiling analysis, we estimate the following time distribution for the sequential implementation:

\begin{table}[h]
\centering
\caption{Estimated Execution Time Breakdown (100 MB Dataset)}
\begin{tabular}{|l|r|r|p{6cm}|}
\hline
\textbf{Operation} & \textbf{Time (ms)} & \textbf{Percentage} & \textbf{Parallelizable?} \\
\hline
String Normalization & 1,043-1,173 & 40-45\% & \textcolor{green}{\textbf{YES}} - Independent per word \\
\hline
File I/O & 652-782 & 25-30\% & \textcolor{orange}{\textbf{PARTIAL}} - Can chunk file \\
\hline
Hash Map Updates & 521-652 & 20-25\% & \textcolor{orange}{\textbf{PARTIAL}} - Needs synchronization \\
\hline
Sorting \& Output & 130-261 & 5-10\% & \textcolor{green}{\textbf{YES}} - Parallel sort possible \\
\hline
\textbf{Total} & \textbf{2,608} & \textbf{100\%} & --- \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Parallelization Strategy Based on Bottlenecks:}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Target: String Normalization (40-45\%)}
        \begin{itemize}[itemsep=2pt]
            \item \textcolor{green}{\textbf{HIGH PRIORITY}} - Largest bottleneck
            \item Strategy: Each thread processes independent text chunks
            \item Expected speedup: Near-linear with thread count
        \end{itemize}
    
    \item \textbf{Target: File I/O (25-30\%)}
        \begin{itemize}[itemsep=2pt]
            \item \textcolor{orange}{\textbf{MEDIUM PRIORITY}} - Can be partially parallelized
            \item Strategy: Memory-map file and divide into chunks
            \item Expected speedup: Limited by disk bandwidth
        \end{itemize}
    
    \item \textbf{Target: Hash Map Updates (20-25\%)}
        \begin{itemize}[itemsep=2pt]
            \item \textcolor{red}{\textbf{CRITICAL SECTION}} - Requires careful synchronization
            \item Strategy: Thread-local hash maps + final merge
            \item Expected speedup: Good, but merge adds overhead
        \end{itemize}
    
    \item \textbf{Target: Sorting (5-10\%)}
        \begin{itemize}[itemsep=2pt]
            \item \textcolor{blue}{\textbf{LOW PRIORITY}} - Minimal impact
            \item Strategy: Use \texttt{std::sort} with execution policy (C++17)
            \item Expected speedup: Marginal benefit
        \end{itemize}
\end{enumerate}

\newpage
\subsection{Scalability Analysis}

\subsubsection{Linear Scaling Verification}

To verify the expected $O(n)$ time complexity, we analyzed the relationship between file size and execution time:

\begin{table}[h]
\centering
\caption{Scalability Metrics}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Size Increase} & \textbf{Time Increase} & \textbf{Scaling Efficiency} & \textbf{Assessment} \\
\hline
10 MB → 25 MB (2.5×) & 2.33× & 93.2\% & Excellent \\
\hline
25 MB → 50 MB (2.0×) & 1.98× & 99.0\% & Excellent \\
\hline
50 MB → 100 MB (2.0×) & 2.05× & 97.5\% & Excellent \\
\hline
10 MB → 100 MB (10.0×) & 9.46× & 94.6\% & Excellent \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Conclusion:} The sequential implementation demonstrates \textbf{excellent linear scaling} with an average efficiency of 94.6\% across all dataset sizes.

\subsubsection{Performance Baseline Summary}

\begin{itemize}[leftmargin=*, itemsep=4pt]
    \item \textbf{Baseline Performance (100 MB):} 2,607.80 ms (2.61 seconds)
    \item \textbf{Processing Rate:} 38.57 MB/s or 7.1 million words/second
    \item \textbf{Scalability:} Linear $O(n)$ with 94.6\% efficiency
    \item \textbf{Primary Bottleneck:} String normalization (40-45\% of time)
    \item \textbf{Parallelization Potential:} 85-90\% of code is parallelizable
\end{itemize}

\vspace{0.5cm}
\noindent\textbf{Expected Parallel Performance (Theoretical):}

Using \textbf{Amdahl's Law} with $P = 0.85$ (85\% parallelizable):

\begin{equation}
S(p) = \frac{1}{(1-P) + \frac{P}{p}} = \frac{1}{0.15 + \frac{0.85}{p}}
\end{equation}

\begin{table}[h]
\centering
\caption{Theoretical Speedup Predictions}
\small
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Threads} & \textbf{Speedup} & \textbf{Time (100 MB)} & \textbf{Improvement} \\
\hline
1 (Sequential) & 1.00× & 2,607.80 ms & --- \\
\hline
2 & 1.77× & 1,473 ms & 43.5\% faster \\
\hline
4 & 3.08× & 847 ms & 67.5\% faster \\
\hline
8 & 4.71× & 554 ms & 78.8\% faster \\
\hline
16 & 5.93× & 440 ms & 83.1\% faster \\
\hline
\end{tabular}
\end{table}

\noindent These predictions will be validated in the next stage through parallel implementation and benchmarking.

\newpage
% ============================================================
\section{Parallel Implementation}
% ============================================================

\subsection{Parallelization Strategy and Methodology}

\subsubsection{OpenMP Framework Selection}

We selected \textbf{OpenMP (Open Multi-Processing)} as our parallelization framework because:

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Shared-Memory Model:} Ideal for multi-core CPUs with shared memory
    \item \textbf{Directive-Based:} Minimal code changes with \texttt{\#pragma} directives
    \item \textbf{Portable:} Cross-platform support (Windows, Linux, macOS)
    \item \textbf{Performance:} Low overhead, efficient thread management
    \item \textbf{Scalability:} Dynamic thread scheduling and load balancing
\end{itemize}

\subsubsection{Parallel Algorithm Design}

Our parallel implementation uses a \textbf{chunk-based parallelization strategy}:

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Data Partitioning:} Divide input words into chunks
    \item \textbf{Thread-Local Processing:} Each thread maintains private hash map
    \item \textbf{Parallel Execution:} Process chunks independently
    \item \textbf{Synchronized Merge:} Combine thread-local results
\end{enumerate}

\newpage
\vspace{0.3cm}
\noindent\textbf{Core Parallel Loop:}

\begin{lstlisting}[language=C++, caption={OpenMP Parallel Word Counting Implementation}]
WordCounterParallel::WordMap 
WordCounterParallel::buildWordMapFromList(
    const std::vector<std::string>& rawWords) {
    
    WordMap wordFreq;  // Global result map
    unsigned long long totalWordCount = 0;
    
    #pragma omp parallel reduction(+ : totalWordCount)
    {
        WordMap localMap;  // Thread-private hash map
        
        #pragma omp for schedule(static)
        for (int i = 0; i < rawWords.size(); ++i) {
            std::string normalized = normalizeWord(rawWords[i]);
            if (!normalized.empty()) {
                localMap[normalized]++;  // Local update (no sync needed)
                totalWordCount++;        // Reduction variable
            }
        }
        
        // Merge phase - synchronized
        #pragma omp critical
        {
            for (const auto& entry : localMap) {
                wordFreq[entry.first] += entry.second;
            }
        }
    }
    
    totalWords = totalWordCount;
    return wordFreq;
}
\end{lstlisting}

\newpage
\subsubsection{Key Parallelization Techniques}

\noindent\textbf{1. Thread-Local Storage}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Each thread maintains private \texttt{localMap}
    \item Eliminates synchronization during word counting
    \item Minimizes contention and false sharing
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{2. OpenMP Reduction Clause}

\begin{lstlisting}[language=C++]
#pragma omp parallel reduction(+ : totalWordCount)
\end{lstlisting}

\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Automatically aggregates thread-local counters
    \item Hardware-optimized, lock-free implementation
    \item Minimal overhead compared to manual synchronization
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{3. Static Scheduling}

\begin{lstlisting}[language=C++]
#pragma omp for schedule(static)
\end{lstlisting}

\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Divides work into equal-sized chunks
    \item Predictable load distribution
    \item Low scheduling overhead
    \item Optimal for uniform workload (our case)
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{4. Critical Section for Merge}

\begin{lstlisting}[language=C++]
#pragma omp critical
{
    for (const auto& entry : localMap) {
        wordFreq[entry.first] += entry.second;
    }
}
\end{lstlisting}

\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Protects shared \texttt{wordFreq} map during merge
    \item \texttt{std::unordered\_map} is not thread-safe
    \item Coarse-grained synchronization (one lock per thread)
    \item Merge complexity: $O(U)$ per thread where $U$ = unique words
\end{itemize}

\newpage
\subsection{Synchronization Methods Implemented}

To address race conditions and study their performance impact, we implemented \textbf{three synchronization methods}:

\subsubsection{Method 1: Reduction (Default)}

\begin{lstlisting}[language=C++, caption={Reduction-Based Synchronization}]
#pragma omp parallel reduction(+ : totalWordCount)
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        // ... process word
        totalWordCount++;  // Automatic reduction
    }
    // ... merge
}
\end{lstlisting}

\noindent\textbf{Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Compiler-optimized aggregation
    \item No explicit locks or atomics
    \item Best performance for counter operations
    \item \textbf{Recommended approach}
\end{itemize}

\subsubsection{Method 2: Atomic Operations}

\begin{lstlisting}[language=C++, caption={Atomic Synchronization}]
#pragma omp parallel
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        // ... process word
        #pragma omp atomic
        totalWordCount++;  // Atomic increment
    }
    // ... merge
}
\end{lstlisting}

\noindent\textbf{Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Hardware-level atomic instruction
    \item Fine-grained synchronization
    \item Good performance for simple operations
    \item Higher overhead than reduction
\end{itemize}

\newpage
\subsubsection{Method 3: Critical Sections}

\begin{lstlisting}[language=C++, caption={Critical Section Synchronization}]
#pragma omp parallel
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        // ... process word
        #pragma omp critical
        totalWordCount++;  // Mutex-protected increment
    }
    // ... merge
}
\end{lstlisting}

\noindent\textbf{Characteristics:}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Serializes access to critical section
    \item High contention with frequent updates
    \item Slowest method
    \item Included for comparison and education
\end{itemize}

\subsection{Race Condition Analysis}

\subsubsection{Identified Race Conditions}

\noindent\textbf{Race Variable 1: wordFreq (std::unordered\_map)}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Problem:} Concurrent updates corrupt hash table
    \item \textbf{Solution:} Thread-local maps + synchronized merge
    \item \textbf{Evidence:} Program crashes without synchronization
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Race Variable 2: totalWords (unsigned long long)}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Problem:} Lost updates from concurrent increments
    \item \textbf{Solution:} Reduction/atomic/critical synchronization
    \item \textbf{Evidence:} Inconsistent word counts without sync
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Race Variable 3: executionTime (double)}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Problem:} Multiple threads write timing data
    \item \textbf{Solution:} Local timing with single final write
    \item \textbf{Evidence:} Non-deterministic timing values
\end{itemize}

\newpage
\subsubsection{Synchronization Method Comparison}

\begin{table}[h]
\centering
\caption{Race Condition Fixes: 10MB Dataset, 4 Threads}
\small
\begin{tabular}{|l|r|r|r|c|}
\hline
\textbf{Method} & \textbf{Time (ms)} & \textbf{Overhead} & \textbf{Correctness} & \textbf{Recommendation} \\
\hline
Reduction & 238.45 & --- & $\checkmark$ Correct & \textcolor{green}{\textbf{Best}} \\
\hline
Atomic & 257.29 & +7.9\% & $\checkmark$ Correct & \textcolor{orange}{Good} \\
\hline
Critical & 431.05 & +80.8\% & $\checkmark$ Correct & \textcolor{red}{Avoid} \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Key Findings:}
\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item All methods guarantee correctness
    \item Reduction achieves best performance
    \item Critical sections introduce severe overhead
    \item Choice of synchronization critically impacts performance
\end{itemize}

\newpage
% ============================================================
\section{Parallel Performance Results}
% ============================================================

\subsection{Benchmarking Methodology}

\subsubsection{Experimental Setup}

\begin{table}[h]
\centering
\caption{Parallel Benchmarking Configuration}
\begin{tabular}{|l|l|}
\hline
\textbf{Parameter} & \textbf{Configuration} \\
\hline
Thread Counts & 1, 2, 4, 8 threads \\
\hline
Sync Methods & Reduction, Atomic, Critical \\
\hline
Datasets & 10MB, 25MB, 50MB, 100MB \\
\hline
Runs per Config & 5 iterations (statistical reliability) \\
\hline
Compiler Flags & -O3 -fopenmp -std=c++17 \\
\hline
Total Tests & 240 benchmark runs \\
\hline
\end{tabular}
\end{table}

\subsubsection{Performance Metrics}

We measure three key metrics to quantify parallel performance:

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Speedup} $S(p)$: Ratio of sequential to parallel execution time
        \[S(p) = \frac{T_{\text{sequential}}}{T_{\text{parallel}}(p)}\]
        where $p$ is the number of threads. Ideal speedup is $S(p) = p$ (linear scaling).
    
    \item \textbf{Efficiency} $E(p)$: Percentage of ideal speedup achieved per processor
        \[E(p) = \frac{S(p)}{p} \times 100\%\]
        Ideal efficiency is 100\%, indicating perfect parallelization. Values below 100\% indicate overhead from synchronization, load imbalance, or sequential bottlenecks.
    
    \item \textbf{Strong Scalability}: How speedup changes with increasing thread count for fixed problem size. Strong scaling shows whether adding more processors continues to improve performance.
\end{enumerate}

\newpage
\subsection{Speedup and Efficiency Analysis}

\subsubsection{Best Performance Results}

\begin{table}[h]
\centering
\caption{Best Speedup Configurations (Reduction Method)\protect\footnotemark}
\small
\begin{tabular}{|l|r|r|r|r|r|}
\hline
\textbf{Dataset} & \textbf{Threads} & \textbf{Seq Time (ms)} & \textbf{Par Time (ms)} & \textbf{Speedup} & \textbf{Efficiency} \\
\hline
test\_10mb.txt & 8 & 572.65 & 316.28 & 1.81× & 22.6\% \\
\hline
test\_25mb.txt & 4 & 783.36 & 712.33 & 1.10× & 27.5\% \\
\hline
test\_50mb.txt & 4 & 1,442.26 & 1,327.29 & 1.09× & 27.2\% \\
\hline
test\_100mb.txt & 8 & 2,849.04 & 2,447.08 & 1.16× & 14.5\% \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Key Observations:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Modest Speedups:} 1.09× to 1.81× improvement
    \item \textbf{Best Configuration:} 10MB dataset with 8 threads (1.81×)
    \item \textbf{Efficiency Concerns:} 14.5\% to 27.5\% efficiency
    \item \textbf{Diminishing Returns:} Limited gains beyond 4 threads
\end{itemize}

\subsubsection{Speedup vs Thread Count}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{speedup_analysis.png}
\caption{Speedup vs Thread Count across datasets and synchronization methods. Red dashed line shows ideal linear speedup for comparison.}
\label{fig:speedup_analysis}
\end{figure}

\newpage
\footnotetext{Sequential baseline times differ from Section 2 measurements due to different code paths: Section 2 uses file-stream processing while parallel benchmarks use list-based pipeline (\texttt{buildWordMapFromList}) for fair comparison with thread-local aggregation.}
\subsubsection{Efficiency Analysis}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{efficiency_analysis.png}
\caption{Parallel Efficiency vs Thread Count. Shows declining efficiency as thread count increases, from 51.2\% with 2 threads down to 12.7\% with 8 threads, demonstrating the impact of sequential bottlenecks and synchronization overhead.}
\label{fig:efficiency_analysis}
\end{figure}

\newpage
\subsection{Synchronization Method Comparison}

\begin{table}[h]
\centering
\caption{Execution Time by Synchronization Method (100MB, 8 Threads)}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Sync Method} & \textbf{Mean Time (ms)} & \textbf{Speedup} & \textbf{vs Reduction} \\
\hline
Reduction & 2,447.08 & 1.16× & --- \\
\hline
Atomic & 2,521.01 & 1.13× & +3.0\% slower \\
\hline
Critical & 5,034.52 & 0.57× & +105.7\% slower \\
\hline
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{sync_method_comparison.png}
\caption{Execution Time Comparison: Synchronization Methods across datasets and thread counts. Critical synchronization (orange) shows significantly higher overhead compared to Reduction (green) and Atomic (blue) methods.}
\label{fig:sync_method_comparison}
\end{figure}

\noindent\textbf{Analysis:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Reduction} achieves best performance consistently
    \item \textbf{Atomic} is competitive (within 3\% of reduction)
    \item \textbf{Critical} doubles execution time due to contention
    \item Synchronization choice matters more than thread count
\end{itemize}

\newpage
\subsection{Scalability Discussion}

\subsubsection{Strong Scaling Analysis}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{scalability_analysis.png}
\caption{Strong Scalability: Execution time vs thread count for 100MB dataset. Green dashed line shows ideal scaling curve.}
\label{fig:scalability}
\end{figure}

\newpage
\noindent\textbf{Scalability Findings:}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Sublinear Scaling:} Performance gains plateau after 4 threads
    
    \item \textbf{Overhead Sources:}
        \begin{itemize}[itemsep=2pt]
            \item Merge phase synchronization (critical section)
            \item Thread creation and management overhead
            \item Load imbalance in final iterations
            \item Cache coherence traffic between cores
        \end{itemize}
    
    \item \textbf{Amdahl's Law Validation:}
        \begin{itemize}[itemsep=2pt]
            \item Measured parallelizable fraction: $P \approx 0.45$
            \item Theoretical max speedup: $S_{\max} = \frac{1}{1-0.45} \approx 1.82×$
            \item Observed max speedup: 1.81× (matches theory)
            \item \textbf{Theory→Practice Bridge:} Our initially predicted $P \approx 0.85$ (Section 2.4) dropped to $P \approx 0.45$ in practice due to unmeasured I/O overhead (26.6\%) and merge synchronization costs (14.3\%) that were not apparent in sequential profiling.
        \end{itemize}
    
    \item \textbf{Efficiency Decline:}
        \begin{itemize}[itemsep=2pt]
            \item 2 threads: 51.2\% efficiency
            \item 4 threads: 26.1\% efficiency
            \item 8 threads: 12.7\% efficiency
        \end{itemize}
\end{enumerate}

\subsubsection{Bottleneck Analysis}

\begin{table}[h]
\centering
\caption{Parallel Execution Time Breakdown (100MB, 8 Threads)}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Phase} & \textbf{Time (ms)} & \textbf{Percentage} \\
\hline
File Reading & 650 & 26.6\% \\
\hline
Parallel Processing & 1,350 & 55.2\% \\
\hline
Merge Synchronization & 350 & 14.3\% \\
\hline
Result Sorting & 97 & 4.0\% \\
\hline
\textbf{Total} & \textbf{2,447} & \textbf{100\%} \\
\hline
\end{tabular}
\end{table}

\noindent\textbf{Critical Bottlenecks:}

\begin{enumerate}[leftmargin=*, itemsep=3pt]
    \item \textbf{File I/O (26.6\%):} Sequential bottleneck, not parallelizable
    \item \textbf{Merge Phase (14.3\%):} Synchronized, limits scalability
    \item \textbf{Small Dataset Effect:} Overhead dominates for small files
\end{enumerate}

\newpage
% ============================================================
\section{Discussion and Limitations}
% ============================================================

\subsection{Performance Analysis}

\subsubsection{Why Speedups Are Modest}

Our parallel implementation achieved 1.09× to 1.81× speedup, which is \textbf{below theoretical predictions}. Key factors:

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Sequential Bottlenecks (Amdahl's Law):}
        \begin{itemize}[itemsep=2pt]
            \item File I/O: 26.6\% of execution time (not parallelizable)
            \item Merge phase: 14.3\% (synchronized bottleneck)
            \item Combined sequential fraction: $\approx 55\%$
            \item Theoretical maximum speedup: $S_{\max} = \frac{1}{0.55} = 1.82×$
        \end{itemize}
    
    \item \textbf{Synchronization Overhead:}
        \begin{itemize}[itemsep=2pt]
            \item Critical section for hash map merge
            \item Thread-local map allocation and deallocation
            \item Barrier synchronization at loop boundaries
        \end{itemize}
    
    \item \textbf{Cache Effects:}
        \begin{itemize}[itemsep=2pt]
            \item Cache coherence traffic between cores
            \item False sharing in shared data structures
            \item Random memory access patterns in hash maps
        \end{itemize}
    
    \item \textbf{Load Imbalance:}
        \begin{itemize}[itemsep=2pt]
            \item Static scheduling with potential uneven workload
            \item Merge phase work varies by unique word count per thread
        \end{itemize}
\end{enumerate}

\subsubsection{Comparison with Expectations}

\begin{table}[h]
\centering
\caption{Expected vs Actual Performance (100MB, 8 Threads)}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Metric} & \textbf{Expected} & \textbf{Actual} & \textbf{Explanation} \\
\hline
Speedup & 4.71× & 1.16× & Sequential bottlenecks \\
\hline
Efficiency & 58.9\% & 14.5\% & Synchronization overhead \\
\hline
Parallelizable \% & 85\% & 45\% & I/O and merge costs \\
\hline
\end{tabular}
\end{table}

\newpage
\subsection{Challenges and Debugging}

\subsubsection{Race Conditions Encountered}

\noindent\textbf{Challenge 1: Hash Map Corruption}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Symptom:} Program crashes with "trace trap" error
    \item \textbf{Cause:} Concurrent updates to \texttt{std::unordered\_map}
    \item \textbf{Solution:} Thread-local maps with synchronized merge
    \item \textbf{Lesson:} Always use thread-safe data structures or synchronization
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Challenge 2: Lost Counter Updates}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Symptom:} Total word count varies between runs
    \item \textbf{Cause:} Unsynchronized increment operations
    \item \textbf{Solution:} Reduction clause for counter aggregation
    \item \textbf{Lesson:} Prefer reduction over manual synchronization
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Challenge 3: Non-Deterministic Timing}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Symptom:} Execution time measurements inconsistent
    \item \textbf{Cause:} Multiple threads writing to shared variable
    \item \textbf{Solution:} Local timing with single final write
    \item \textbf{Lesson:} Minimize shared writes in performance-critical paths
\end{itemize}

\subsubsection{Debugging Techniques Used}

\begin{enumerate}[leftmargin=*, itemsep=3pt]
    \item \textbf{OpenMP Thread Sanitizer:} Detect data races
    \item \textbf{Validation Runs:} Compare parallel vs sequential outputs
    \item \textbf{Performance Profiling:} Identify synchronization bottlenecks
    \item \textbf{Iterative Testing:} Test with different thread counts and datasets
\end{enumerate}

\newpage
\subsection{Limitations of Current Approach}

\subsubsection{Technical Limitations}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Sequential I/O Bottleneck:}
        \begin{itemize}[itemsep=2pt]
            \item File reading remains sequential
            \item Dominates execution time for small-medium files
            \item \textbf{Impact:} Limits maximum achievable speedup to $<2×$
        \end{itemize}
    
    \item \textbf{Merge Phase Overhead:}
        \begin{itemize}[itemsep=2pt]
            \item Hash map merge requires synchronization
            \item Complexity: $O(p \times U)$ where $p$ = threads, $U$ = unique words
            \item \textbf{Impact:} 14\% of execution time with 8 threads
        \end{itemize}
    
    \item \textbf{Memory Overhead:}
        \begin{itemize}[itemsep=2pt]
            \item Each thread maintains separate hash map
            \item Memory usage: $O(p \times U \times \text{avg\_word\_len})$
            \item \textbf{Impact:} Higher memory footprint vs sequential
        \end{itemize}
    
    \item \textbf{Limited Scalability:}
        \begin{itemize}[itemsep=2pt]
            \item Efficiency drops below 15\% with 8 threads
            \item Adding more threads provides minimal benefit
            \item \textbf{Impact:} Not cost-effective beyond 4 threads
        \end{itemize}
\end{enumerate}

\subsubsection{Algorithmic Limitations}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Static Scheduling:}
        \begin{itemize}[itemsep=2pt]
            \item Assumes uniform work distribution
            \item Cannot adapt to runtime load imbalance
            \item Alternative: Dynamic scheduling (higher overhead)
        \end{itemize}
    
    \item \textbf{Shared-Memory Constraint:}
        \begin{itemize}[itemsep=2pt]
            \item Limited to single-machine parallelism
            \item Cannot scale to distributed systems
            \item Alternative: MPI-based distributed implementation
        \end{itemize}
\end{enumerate}

\newpage
\subsection{Future Work and Improvements}

\subsubsection{Short-Term Improvements}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Parallel I/O:}
        \begin{itemize}[itemsep=2pt]
            \item Memory-map file and divide into chunks
            \item Parallel read with \texttt{mmap()} or similar
            \item \textbf{Expected Gain:} 20-30\% speedup improvement
        \end{itemize}
    
    \item \textbf{Lock-Free Hash Map:}
        \begin{itemize}[itemsep=2pt]
            \item Use concurrent hash map (e.g., \texttt{tbb::concurrent\_hash\_map})
            \item Eliminate merge phase synchronization
            \item \textbf{Expected Gain:} 10-15\% speedup improvement
        \end{itemize}

    
    \item \textbf{Dynamic Scheduling:}
\begin{lstlisting}[language=C++]
#pragma omp for schedule(dynamic, chunk_size)
\end{lstlisting}
        \begin{itemize}[itemsep=2pt]
            \item Adapt to runtime load imbalance
            \item Better performance for variable-length words
            \item \textbf{Expected Gain:} 5-10\% for non-uniform data
        \end{itemize}
\end{enumerate}

\newpage
\subsubsection{Long-Term Research Directions}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item \textbf{Hybrid Parallelism:}
        \begin{itemize}[itemsep=2pt]
            \item Combine OpenMP (shared-memory) + MPI (distributed)
            \item Scale to multiple machines
            \item Target: Process TB-scale datasets
        \end{itemize}
    
    \item \textbf{GPU Acceleration:}
        \begin{itemize}[itemsep=2pt]
            \item Port to CUDA/OpenCL
            \item Leverage thousands of GPU cores
            \item Target: 10-50× speedup for large files
        \end{itemize}
    
    \item \textbf{Advanced Algorithms:}
        \begin{itemize}[itemsep=2pt]
            \item Parallel trie-based counting
            \item Approximate counting (Count-Min Sketch)
            \item Streaming algorithms for memory efficiency
        \end{itemize}
    
    \item \textbf{Real-World Applications:}
        \begin{itemize}[itemsep=2pt]
            \item Integrate with Apache Spark/Hadoop
            \item Apply to social media analytics
            \item Extend to N-gram frequency analysis
        \end{itemize}
\end{enumerate}

\newpage
% ============================================================
\section{Template Usage and Documentation}
% ============================================================

\subsection{Template Completion}

\subsubsection{Project Completion Status}

\textbf{All project stages have been successfully completed:}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Introduction (Section 1):}
    \begin{itemize}[itemsep=2pt]
        \item Objective and Purpose
        \item Code Selection and Justification
    \end{itemize}
    
    \item \textbf{Sequential Benchmarking (Section 2):}
    \begin{itemize}[itemsep=2pt]
        \item Benchmarking Methodology
        \item Performance Results (4 datasets: 10MB, 25MB, 50MB, 100MB)
        \item Bottleneck Identification (I/O, string processing, hash map operations)
        \item Scalability Predictions
    \end{itemize}
    
    \item \textbf{Parallel Implementation (Section 3):}
    \begin{itemize}[itemsep=2pt]
        \item OpenMP Parallelization Strategy (thread-local maps, reduction clause)
        \item Three Synchronization Methods (reduction, atomic, critical)
        \item Race Condition Analysis (wordFreq, totalWords, executionTime)
        \item Complete Code Implementation with 24 listings
    \end{itemize}
    
    \item \textbf{Parallel Performance Results (Section 4):}
    \begin{itemize}[itemsep=2pt]
        \item Comprehensive Benchmarking (240 tests: 4 datasets × 3 sync methods × 4 threads × 5 runs)
        \item Speedup Analysis (best: 1.81× with 8 threads)
        \item Efficiency Analysis (12.7\% to 51.2\% across thread counts)
        \item Synchronization Method Comparison (reduction > atomic > critical)
        \item Strong Scalability Discussion with Amdahl's Law validation
    \end{itemize}
    
    \item \textbf{Discussion and Limitations (Section 5):}
    \begin{itemize}[itemsep=2pt]
        \item Performance Analysis (sequential bottlenecks, cache effects)
        \item Challenges and Debugging (race conditions, synchronization)
        \item Technical Limitations (I/O bottleneck, merge overhead)
        \item Future Work (parallel I/O, lock-free hash maps, GPU acceleration)
    \end{itemize}

    \newpage
    
    \item \textbf{Template Usage and Documentation (Section 6):}
    \begin{itemize}[itemsep=2pt]
        \item Template Completion Status
        \item Code Visualization and Repository Links
        \item Document Formatting and LaTeX Guidelines
    \end{itemize}
    
    \item \textbf{References (Section 7):}
    \begin{itemize}[itemsep=2pt]
        \item 8 Academic and Technical Citations
        \item OpenMP Specification, Amdahl's Law, Concurrency Literature
    \end{itemize}
    
    \item \textbf{Appendix (Section 8):}
    \begin{itemize}[itemsep=2pt]
        \item Complete Project Repository Structure
        \item Build and Run Instructions (sequential + parallel)
        \item Performance Metrics Definitions
        \item Synchronization Code Examples
        \item Acknowledgments and Tools Used
    \end{itemize}
\end{enumerate}

\newpage
\subsection{Code Visualization and Repository}

\subsubsection{Project Structure}

Instead of screenshots, we provide a \textbf{live GitHub repository} with complete source code:

\begin{center}
\Large\textbf{GitHub Repository:} \\
\url{https://github.com/0x00K1/Parallel-Grepper}
\end{center}

\textbf{Repository Structure:}
\begin{verbatim}
Parallel-Grepper/
|-- docs/                         # Documentation and proposal
|-- src/
|   |-- sequential/               # Sequential implementation
|   +-- parallel/                 # Parallel implementation (OpenMP)
|-- benchmarks/                   # Benchmarking scripts and results
|-- data/                         # Test datasets (10MB to 100MB+)
|-- scripts/                      # Utility scripts
+-- results/                      # Output word frequency results
\end{verbatim}

\subsubsection{Code Access Methods}

\textbf{Method 1: Direct File Links}
\begin{itemize}[leftmargin=*]
    \item Sequential Header: \\
    \small\url{https://github.com/0x00K1/Parallel-Grepper/blob/main/src/sequential/word_counter_sequential.h}
    \item Sequential Implementation: \\
    \small\url{https://github.com/0x00K1/Parallel-Grepper/blob/main/src/sequential/word_counter_sequential.cpp}
    \item Parallel Header: \\
    \small\url{https://github.com/0x00K1/Parallel-Grepper/blob/main/src/parallel/word_counter_parallel.h}
    \item Parallel Implementation: \\
    \small\url{https://github.com/0x00K1/Parallel-Grepper/blob/main/src/parallel/word_counter_parallel.cpp}
\end{itemize}

\newpage
\textbf{Method 2: Clone Repository}
\begin{lstlisting}[language=bash, caption={Clone and Build Instructions}]
# Clone repository
git clone https://github.com/0x00K1/Parallel-Grepper.git
cd Parallel-Grepper

# Build sequential version
mkdir build
g++ -std=c++17 -O3 -o build/sequential_counter \
    src/sequential/word_counter_sequential.cpp \
    src/sequential/main.cpp

# Build parallel version
g++ -std=c++17 -O3 -fopenmp -o build/parallel_counter \
    src/parallel/word_counter_parallel.cpp \
    src/parallel/main.cpp

# Generate test datasets
python benchmarks/generate_dataset.py
\end{lstlisting}

\subsubsection{Code Functionality Explanation}

\noindent\textbf{1. Sequential Word Counter Class:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Purpose:} Process text files and count word frequencies
    \item \textbf{Input:} Text file path or string content
    \item \textbf{Output:} Hash map of word frequencies
    \item \textbf{Key Methods:}
        \begin{itemize}[itemsep=2pt]
            \item \texttt{countWordsFromFile()}: Process file and return frequency map
            \item \texttt{normalizeWord()}: Convert to lowercase, remove punctuation
            \item \texttt{getTopWords()}: Extract most frequent words
            \item \texttt{saveResults()}: Export results to file
        \end{itemize}
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{2. Benchmarking Infrastructure:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Dataset Generator:} Creates realistic text files with Zipf-like word distribution
    \item \textbf{Benchmark Runner:} Executes multiple runs, collects statistics
    \item \textbf{Performance Analyzer:} Generates graphs and summary reports
\end{itemize}

\newpage

\noindent\textbf{3. Workflow:}

\begin{enumerate}[leftmargin=*, itemsep=4pt]
    \item Generate test datasets (10MB - 100MB)
    \item Compile sequential version with optimizations
    \item Run 5 iterations per dataset
    \item Collect timing statistics (mean, median, std dev)
    \item Analyze bottlenecks and identify parallelization targets
    \item Generate performance visualizations
\end{enumerate}

\subsection{Document Formatting and Compilation}

\subsubsection{LaTeX Formatting and Compilation}

This document is written in \textbf{LaTeX} using Overleaf.

\subsection{Project Implementation Summary}

\subsubsection{Parallel Implementation Approach}

The parallel implementation successfully employs a \textbf{chunk-based parallelization strategy with thread-local storage}, as detailed in Section 3. The implementation uses OpenMP 5.x with the following key techniques:

\vspace{0.3cm}
\noindent\textbf{Implemented Strategy: Thread-Local Maps with Reduction}

\begin{lstlisting}[language=C++, caption={Implemented OpenMP Parallel Word Counter}]
#pragma omp parallel reduction(+ : totalWordCount)
{
    WordMap localMap;  // Thread-local hash map
    
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        std::string normalized = normalizeWord(rawWords[i]);
        if (!normalized.empty()) {
            localMap[normalized]++;  // No sync needed
            totalWordCount++;        // Reduction aggregation
        }
    }
    
    // Merge thread-local maps into global result
    #pragma omp critical
    {
        for (const auto& entry : localMap) {
            wordFreq[entry.first] += entry.second;
        }
    }
}
\end{lstlisting}

\vspace{0.5cm}
\noindent\textbf{Key Implementation Features:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item \textbf{Thread-Local Storage:} Each thread maintains private hash map
    \item \textbf{Zero Contention:} No synchronization during word counting phase
    \item \textbf{Reduction Clause:} Optimal counter aggregation (best performance)
    \item \textbf{Critical Merge:} Synchronized final combination of results
    \item \textbf{Static Scheduling:} Predictable load distribution for uniform workload
\end{itemize}

\vspace{0.5cm}
\noindent\textbf{Performance Achievements:}

\begin{itemize}[leftmargin=*, itemsep=3pt]
    \item Best speedup: 1.81× (10MB dataset, 8 threads)
    \item Reduction synchronization: 105\% faster than critical sections
    \item Amdahl's Law validation: theoretical max 1.82×, achieved 1.81×
    \item 240 comprehensive benchmarks (4 datasets × 3 sync methods × 4 threads × 5 runs)
\end{itemize}

\vspace{0.5cm}
\noindent For complete implementation details, see Section 3 (Parallel Implementation) and Section 4 (Performance Results).

% ============================================================
\section{References}
% ============================================================

\begin{enumerate}[leftmargin=*]
    \item OpenMP Architecture Review Board. \textit{OpenMP Application Programming Interface Version 5.2}. November 2021. \url{https://www.openmp.org/specifications/}
    
    \item Chapman, Barbara, Gabriele Jost, and Ruud van der Pas. \textit{Using OpenMP: Portable Shared Memory Parallel Programming}. MIT Press, 2007.
    
    \item Herlihy, Maurice, and Nir Shavit. \textit{The Art of Multiprocessor Programming}. Morgan Kaufmann, 2012.
    
    \item Pacheco, Peter. \textit{An Introduction to Parallel Programming}. Morgan Kaufmann, 2011.
    
    \item Amdahl, Gene M. \textit{Validity of the single processor approach to achieving large scale computing capabilities}. AFIPS Conference Proceedings, 1967.
    
    \item Williams, Anthony. \textit{C++ Concurrency in Action, 2nd Edition}. Manning Publications, 2019.
    
    \item Dr. Yasir Alguwaifli. \textit{ARTI503 Course Materials - Parallel Computer Architecture}. IAU, 2025.
    
    \item Amazon Web Services. \textit{AWS Cloud Credits for Research}. \url{https://aws.amazon.com/research-credits/}
\end{enumerate}

\newpage
% ============================================================
\section{Appendix}
% ============================================================

\subsection{Appendix A: Project Repository Structure}

\noindent\textbf{Complete directory tree:}

\begin{verbatim}
parallel-grepper/
    src/
        sequential/          # Sequential implementation
            main.cpp
            word_counter_sequential.cpp
            word_counter_sequential.h
        parallel/            # Parallel implementation (OpenMP)  
            main.cpp
            word_counter_parallel.cpp
            word_counter_parallel.h
    benchmarks/
        F-run_sequential_benchmarks.py  # Sequential benchmarks
        F-run_parallel_benchmarks.py    # Parallel benchmarks
        F-analyze_sequential_results.py # Visualization & analysis
        F-analyze_parallel_results.py   # Visualization & analysis
        generate_dataset.py           # Test data generation
        results/                      # Benchmark outputs (CSV/JSON/PNG)
    scripts/
        build.ps1                   # Windows build automation
        build.sh                    # Linux/macOS build script
        run_sync_tests.ps1          # Race condition tests
    docs/
        proposal.tex                # This document
        BUILD_GUIDE.md              # Compilation instructions
        RACE_CONDITION_FIXES.md     # Synchronization method details
    data/                           # Test datasets
        # Generated text files (10MB - 100MB+) from generate_dataset.py
    results/
        sequential/                 # Sequential outputs
        parallel/                   # Parallel outputs
\end{verbatim}

\newpage
\subsection{Appendix B: Build and Run Instructions}

\textbf{Prerequisites:}
\begin{itemize}[itemsep=2pt]
    \item GCC 15.2.0+ with OpenMP support (MinGW-w64 POSIX UCRT on Windows)
    \item C++17 standard library
    \item Python 3.8+ with pandas, matplotlib, seaborn (for benchmarking)
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Compilation Commands:}

\begin{lstlisting}[language=bash, caption={Sequential Version Build}]
# Create build directory
mkdir -p build results/sequential

# Compile sequential version (Linux/macOS)
g++ -std=c++17 -O3 -march=native \
    -o build/sequential_counter \
    src/sequential/word_counter_sequential.cpp \
    src/sequential/main.cpp

# Windows with MinGW
g++ -std=c++17 -O3 ^
    -o build/sequential_counter.exe ^
    src/sequential/word_counter_sequential.cpp ^
    src/sequential/main.cpp
\end{lstlisting}

\begin{lstlisting}[language=bash, caption={Parallel Version Build}]
# Create build directory
mkdir -p build results/parallel

# Compile parallel version (Linux/macOS)
g++ -std=c++17 -O3 -fopenmp -march=native \
    -o build/parallel_counter \
    src/parallel/word_counter_parallel.cpp \
    src/parallel/main.cpp

# Windows with MinGW
g++ -std=c++17 -O3 -fopenmp ^
    -o build/parallel_counter.exe ^
    src/parallel/word_counter_parallel.cpp ^
    src/parallel/main.cpp
\end{lstlisting}

\newpage
\vspace{0.3cm}
\noindent\textbf{Usage Examples:}

\begin{lstlisting}[language=bash, caption={Running Parallel Counter}]
# Default: 8 threads with reduction synchronization
./build/parallel_counter data/test_100mb.txt

# Specify thread count
./build/parallel_counter data/test_50mb.txt 4

# Specify sync method: reduction, atomic, or critical
./build/parallel_counter data/test_10mb.txt 4 reduction
./build/parallel_counter data/test_10mb.txt 4 atomic
./build/parallel_counter data/test_10mb.txt 4 critical

# Windows PowerShell
.\build\parallel_counter.exe data\test_100mb.txt 8 reduction
\end{lstlisting}

\subsection{Appendix C: Performance Metrics Definitions}

\noindent\textbf{Key Performance Indicators:}

\begin{enumerate}[leftmargin=*, itemsep=8pt]
    \item \textbf{Speedup:}
        \[S(p) = \frac{T_{\text{sequential}}}{T_{\text{parallel}}(p)}\]
        where $p$ is the number of threads
    
    \item \textbf{Efficiency:}
        \[E(p) = \frac{S(p)}{p} \times 100\%\]
        Ideal efficiency is 100\%, indicating perfect scaling
    
    \item \textbf{Throughput:}
        \[\text{Throughput} = \frac{\text{Data Size (MB)}}{\text{Execution Time (seconds)}}\]
    
    \item \textbf{Strong Scalability:}
        \[\text{Fixed problem size, varying thread count}\]
        Measured by how speedup changes as threads increase
    
    \item \textbf{Amdahl's Law:}
        \[S_{\max}(p) = \frac{1}{(1-P) + \frac{P}{p}}\]
        where $P$ is the parallelizable fraction
\end{enumerate}

\newpage
\subsection{Appendix D: Synchronization Method Code Examples}

\noindent\textbf{Reduction (Recommended):}

\begin{lstlisting}[language=C++, caption={Reduction Synchronization}, basicstyle=\scriptsize\ttfamily]
#pragma omp parallel reduction(+ : totalWordCount)
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        std::string normalized = normalizeWord(rawWords[i]);
        if (!normalized.empty()) {
            localMap[normalized]++;
            totalWordCount++;  // Automatic reduction
        }
    }
    // Merge localMap into wordFreq under critical section
}
\end{lstlisting}

\noindent\textbf{Atomic Operations:}

\begin{lstlisting}[language=C++, caption={Atomic Synchronization}, basicstyle=\scriptsize\ttfamily]
#pragma omp parallel
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        std::string normalized = normalizeWord(rawWords[i]);
        if (!normalized.empty()) {
            localMap[normalized]++;
            #pragma omp atomic
            totalWordCount++;  // Atomic increment
        }
    }
    // Merge phase
}
\end{lstlisting}

\noindent\textbf{Critical Sections:}

\begin{lstlisting}[language=C++, caption={Critical Section Synchronization}, basicstyle=\scriptsize\ttfamily]
#pragma omp parallel
{
    WordMap localMap;
    #pragma omp for schedule(static)
    for (int i = 0; i < rawWords.size(); ++i) {
        std::string normalized = normalizeWord(rawWords[i]);
        if (!normalized.empty()) {
            localMap[normalized]++;
            #pragma omp critical
            totalWordCount++;  // Mutex-protected increment
        }
    }
    // Merge phase
}
\end{lstlisting}

\newpage
\subsection{Appendix E: Acknowledgments}

This project was completed as part of the \textbf{Parallel Computer Architecture (ARTI503)} course at \textbf{Imam Abdulrahman Bin Faisal University (IAU)}, College of Computer Science and Information Technology (CCSIT).

\vspace{0.3cm}
\noindent\textbf{Course Details:}
\begin{itemize}[itemsep=2pt]
    \item Fourth Year, Semester 1, Academic Year 2025-2026
    \item Instructor: Dr. Yasir Alguwaifli
    \item Project: Parallel Word Frequency Counter
    \item Framework: OpenMP 5.x for shared-memory parallelization
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Tools and Technologies:}
\begin{itemize}[itemsep=2pt]
    \item C++17 with GCC 15.2.0 (MinGW-w64 POSIX UCRT)
    \item OpenMP 5.x API for parallel programming
    \item Python 3.x with pandas, matplotlib, seaborn for analysis
    \item Windows 11 development environment
    \item LaTeX/Overleaf for documentation
\end{itemize}

\end{document}